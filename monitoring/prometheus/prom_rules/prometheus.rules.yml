# namespace: prometheus.rules
groups:
    - name: scylla.rules
      rules:
        - alert: cqlNonPrepared
          expr: cql:non_prepared > 0
          for: 10s
          labels:
            advisor: cqlOptimization
            dashboard: cql
            description: Some queries are non-prepared
            severity: info
            summary: non prepared statements
        - alert: cql:non_paged_no_system
          expr: cql:non_paged > 0
          for: 10s
          labels:
            advisor: cqlOptimization
            dashboard: cql
            description: Some SELECT queries are non-paged
            severity: info
            status: "1"
            summary: non paged statements
        - alert: cqlNoTokenAware
          expr: cql:non_token_aware > 0
          for: 10s
          labels:
            advisor: cqlOptimization
            dashboard: cql
            description: Some queries are not token-aware
            severity: info
            summary: non token aware statements
        - alert: cqlAllowFiltering
          expr: cql:allow_filtering > 0
          for: 10s
          labels:
            advisor: cqlOptimization
            dashboard: cql
            description: Some queries use ALLOW FILTERING
            severity: info
            summary: Allow filtering queries
        - alert: cqlCLAny
          expr: cql:any_queries > 0
          for: 10s
          labels:
            advisor: cqlOptimization
            dashboard: cql
            description: 'Some queries use Consistency Level: ANY'
            severity: info
            summary: non prepared statements
        - alert: cqlCLAll
          expr: cql:all_queries > 0
          for: 10s
          labels:
            advisor: cqlOptimization
            dashboard: cql
            description: 'Some queries use Consistency Level: ALL'
            severity: info
            summary: non prepared statements
        - alert: nonBalancedcqlTraffic
          expr: abs(rate(scylla_cql_updates{conditional="no"}[1m]) - scalar(avg(rate(scylla_cql_updates{conditional="no"}[1m])))) / scalar(stddev(rate(scylla_cql_updates{conditional="no"}[1m])) + 100) > 2
          for: 3m
          labels:
            advisor: balanced
            dashboard: cql
            description: CQL queries are not balanced among shards {{ $labels.instance }} shard {{ $labels.shard }}
            severity: info
            status: "1"
            summary: CQL queries are not balanced
        - alert: nodeLocalErrors
          expr: sum by (cluster, instance) (errors:local_failed) > 0
          for: 10s
          labels:
            advisor: operationError
            dashboard: scylla-detailed
            description: Some operation failed at the replica side
            severity: info
            summary: Replica side Level error
        - alert: nodeIOErrors
          expr: sum by (cluster, instance) (rate(scylla_reactor_aio_errors[1m])) > 0
          for: 10s
          labels:
            advisor: operationError
            dashboard: OS-master
            description: IO Errors can indicate a node with a faulty disk {{ $labels.instance }}
            severity: info
            summary: IO Disk Error
        - alert: nodeCLErrors
          expr: sum by (cluster) (errors:operation_unavailable) > 0
          for: 10s
          labels:
            advisor: operationError
            dashboard: scylla-detailed
            description: Some operation failed due to consistency level
            severity: info
            summary: Consistency Level error
        - alert: preparedCacheEviction
          expr: sum by (cluster) (rate(scylla_cql_prepared_cache_evictions[2m])) + sum by (cluster) (rate(scylla_cql_authorized_prepared_statements_cache_evictions[2m])) > 100
          for: 5m
          labels:
            advisor: preparedEviction
            dashboard: scylla-detailed
            description: The prepared-statement cache is being continuously evicted, which could indicate a problem in your prepared-statement usage logic.
            severity: info
            summary: Prepared cache eviction
        - alert: heavyCompaction
          expr: max by (cluster) (scylla_scheduler_shares{group="compaction"}) >= 1000
          for: 20m
          labels:
            advisor: heavyCompaction
            dashboard: scylla-detailed
            description: Compaction load increases to a level it can interfere with the system behaviour. If this persists set the compaction share to a static level.
            severity: info
            summary: Heavy compaction load
        - alert: shedRequests
          expr: max by (cluster) (sum by (instance, cluster) (rate(scylla_transport_requests_shed[1m])) / sum by (instance, cluster) (rate(scylla_transport_requests_served[1m]))) > 0.01
          for: 5m
          labels:
            advisor: systemOverload
            dashboard: scylla-detailed
            description: More than 1% of the requests got shed, this is an indication of an overload, consider system resize.
            severity: info
            summary: System is overloaded
        - alert: cappedTombstone
          expr: changes(scylla_sstables_capped_tombstone_deletion_time[1h]) > 0
          for: 1m
          labels:
            advisor: cappedTombstone
            dashboard: scylla-detailed
            description: Tombstone delete time was set too far in the future and was capped
            severity: info
            summary: Tobmstone delete time is capped
        - alert: InstanceDown
          expr: up{job="scylla"} == 0
          for: 30s
          labels:
            severity: error
          annotations:
            description: '{{ $labels.instance }} has been down for more than 30 seconds.'
            summary: Instance {{ $labels.instance }} down
        - alert: InstanceDown
          expr: sum by (instance) (up{job="scylla"} > 0) unless sum by (instance) (scylla_transport_requests_served{shard="0"})
          for: 1m
          labels:
            severity: error
          annotations:
            description: '{{ $labels.instance }} instance is shutting down.'
            summary: Instance {{ $labels.instance }} down
        - alert: InstanceDown
          expr: scylla_node_operation_mode > 3
          for: 30s
          labels:
            severity: error
          annotations:
            description: '{{ $labels.instance }} instance is shutting down.'
            summary: Instance {{ $labels.instance }} down
        - alert: DiskFull
          expr: node_filesystem_avail_bytes{mountpoint="/var/lib/scylla"} / node_filesystem_size_bytes{mountpoint="/var/lib/scylla"} * 100 < 35
          for: 30s
          labels:
            severity: warn
          annotations:
            description: '{{ $labels.instance }} has less than 35% free disk space.'
            summary: Instance {{ $labels.instance }} low disk space
        - alert: DiskFull
          expr: node_filesystem_avail_bytes{mountpoint="/var/lib/scylla"} / node_filesystem_size_bytes{mountpoint="/var/lib/scylla"} * 100 < 25
          for: 30s
          labels:
            severity: error
          annotations:
            description: '{{ $labels.instance }} has less than 25% free disk space.'
            summary: Instance {{ $labels.instance }} low disk space
        - alert: DiskFull
          expr: node_filesystem_avail_bytes{mountpoint="/var/lib/scylla"} / node_filesystem_size_bytes{mountpoint="/var/lib/scylla"} * 100 < 15
          for: 30s
          labels:
            severity: critical
          annotations:
            description: '{{ $labels.instance }} has less than 15% free disk space.'
            summary: Instance {{ $labels.instance }} low disk space
        - alert: DiskFull
          expr: node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"} * 100 < 20
          for: 30s
          labels:
            severity: error
          annotations:
            description: '{{ $labels.instance }} has less than 20% free disk space on the root partition.'
            summary: Instance {{ $labels.instance }} low disk space
        - alert: NoCql
          expr: scylla_manager_healthcheck_cql_status == -1
          for: 5m
          labels:
            severity: info
          annotations:
            description: '{{ $labels.host }} has denied CQL connection for more than 30 seconds.'
            summary: Instance {{ $labels.host }} no CQL connection
        - alert: HighLatencies
          expr: wlatencyp95{by="instance"} > 100000
          for: 5m
          labels:
            severity: info
          annotations:
            description: '{{ $labels.instance }} has 95% high latency for more than 5 minutes.'
            summary: Instance {{ $labels.instance }} High Write Latency
        - alert: HighLatencies
          expr: wlatencya{by="instance"} > 10000
          for: 5m
          labels:
            severity: info
          annotations:
            description: '{{ $labels.instance }} has average high latency for more than 5 minutes.'
            summary: Instance {{ $labels.instance }} High Write Latency
        - alert: HighLatencies
          expr: rlatencyp95{by="instance"} > 100000
          for: 5m
          labels:
            severity: info
          annotations:
            description: '{{ $labels.instance }} has 95% high latency for more than 5 minutes.'
            summary: Instance {{ $labels.instance }} High Read Latency
        - alert: HighLatencies
          expr: rlatencya{by="instance"} > 10000
          for: 5m
          labels:
            severity: info
          annotations:
            description: '{{ $labels.instance }} has average high latency for more than 5 minutes.'
            summary: Instance {{ $labels.instance }} High Read Latency
        - alert: BackupFailed
          expr: (sum(scylla_manager_scheduler_run_total{status="ERROR",type=~"backup"}) or vector(0)) - (sum(scylla_manager_scheduler_run_total{status="ERROR",type=~"backup"} offset 3m) or vector(0)) > 0
          for: 10s
          labels:
            severity: warn
          annotations:
            description: Backup failed
            summary: Backup task failed
        - alert: RepairFailed
          expr: (sum(scylla_manager_scheduler_run_total{status="ERROR",type=~"repair"}) or vector(0)) - (sum(scylla_manager_scheduler_run_total{status="ERROR",type=~"repair"} offset 3m) or vector(0)) > 0
          for: 10s
          labels:
            severity: warn
          annotations:
            description: Repair failed
            summary: Repair task failed
        - alert: restart
          expr: resets(scylla_gossip_heart_beat[1h]) > 0
          for: 10s
          labels:
            severity: info
          annotations:
            description: Node restarted
            summary: Instance {{ $labels.instance }} restarted
        - alert: oomKill
          expr: changes(node_vmstat_oom_kill[1h]) > 0
          for: 10s
          labels:
            severity: warn
          annotations:
            description: OOM Kill on {{ $labels.instance }}
            summary: A process was terminated on Instance {{ $labels.instance }}
        - alert: tooManyFiles
          expr: (node_filesystem_files{mountpoint="/var/lib/scylla"} - node_filesystem_files_free{mountpoint="/var/lib/scylla"}) / on (instance) group_left () count by (instance) (scylla_reactor_cpu_busy_ms) > 20000
          for: 10s
          labels:
            description: Over 20k open files in /var/lib/scylla per shard {{ $labels.instance }}
            severity: info
            summary: There are over 20K open files per shard on Instance {{ $labels.instance }}
        - alert: tooManyFiles
          expr: (node_filesystem_files{mountpoint="/var/lib/scylla"} - node_filesystem_files_free{mountpoint="/var/lib/scylla"}) / on (instance) group_left () count by (instance) (scylla_reactor_cpu_busy_ms) > 30000
          for: 10s
          labels:
            description: Over 30k open files in /var/lib/scylla per shard {{ $labels.instance }}
            severity: warn
            summary: There are over 30K open files per shard on Instance {{ $labels.instance }}
        - alert: tooManyFiles
          expr: (node_filesystem_files{mountpoint="/var/lib/scylla"} - node_filesystem_files_free{mountpoint="/var/lib/scylla"}) / on (instance) group_left () count by (instance) (scylla_reactor_cpu_busy_ms) > 40000
          for: 10s
          labels:
            description: Over 40k open files in /var/lib/scylla per shard {{ $labels.instance }}
            severity: error
            summary: There are over 40K open files per shard on Instance {{ $labels.instance }}
        - alert: nodeInJoinMode
          expr: scylla_node_operation_mode == 2
          for: 5h
          labels:
            description: Node {{ $labels.instance }} in Joining mode for 5 hours
            severity: info
            summary: Node {{ $labels.instance }} in Joining mode for 5 hours
        - alert: nodeInJoinMode
          expr: scylla_node_operation_mode == 2
          for: 1d
          labels:
            description: Node {{ $labels.instance }} in Joining mode for 1 day
            severity: warn
            summary: Node {{ $labels.instance }} in Joining mode for 1 day
        - alert: splitBrain
          expr: sum(scylla_gossip_live) >= (count(scylla_node_operation_mode == 3) - 1) * count(scylla_gossip_live)
          for: 10m
          labels:
            description: Cluster in a split-brain mode
            severity: warn
            summary: Some nodes the cluster do not see all of the other live nodes
